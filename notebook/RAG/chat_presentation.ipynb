{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirement.txt (line 1)) (5.2.1)\n",
      "Collecting atlassian-python-api (from -r ../requirement.txt (line 2))\n",
      "  Using cached atlassian_python_api-3.41.14-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting langchain (from -r ../requirement.txt (line 3))\n",
      "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain_community (from -r ../requirement.txt (line 4))\n",
      "  Downloading langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain_openai (from -r ../requirement.txt (line 5))\n",
      "  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirement.txt (line 6)) (0.21.0)\n",
      "Collecting pymongo (from -r ../requirement.txt (line 7))\n",
      "  Using cached pymongo-4.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirement.txt (line 8)) (10.3.0)\n",
      "Collecting pytesseract (from -r ../requirement.txt (line 9))\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image (from -r ../requirement.txt (line 11))\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting docx2txt (from -r ../requirement.txt (line 12))\n",
      "  Using cached docx2txt-0.8-py3-none-any.whl\n",
      "Collecting reportlab (from -r ../requirement.txt (line 13))\n",
      "  Using cached reportlab-4.2.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting svglib (from -r ../requirement.txt (line 14))\n",
      "  Using cached svglib-1.5.1-py3-none-any.whl\n",
      "Collecting deprecated (from atlassian-python-api->-r ../requirement.txt (line 2))\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from atlassian-python-api->-r ../requirement.txt (line 2)) (2.32.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from atlassian-python-api->-r ../requirement.txt (line 2)) (1.16.0)\n",
      "Collecting oauthlib (from atlassian-python-api->-r ../requirement.txt (line 2))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting requests-oauthlib (from atlassian-python-api->-r ../requirement.txt (line 2))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jmespath in /opt/anaconda3/lib/python3.12/site-packages (from atlassian-python-api->-r ../requirement.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from atlassian-python-api->-r ../requirement.txt (line 2)) (4.12.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain->-r ../requirement.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain->-r ../requirement.txt (line 3)) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain->-r ../requirement.txt (line 3)) (3.9.5)\n",
      "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain->-r ../requirement.txt (line 3))\n",
      "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->-r ../requirement.txt (line 3))\n",
      "  Using cached langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r ../requirement.txt (line 3))\n",
      "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain->-r ../requirement.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain->-r ../requirement.txt (line 3)) (2.5.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain->-r ../requirement.txt (line 3)) (8.2.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community->-r ../requirement.txt (line 4))\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai->-r ../requirement.txt (line 5))\n",
      "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai->-r ../requirement.txt (line 5))\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo->-r ../requirement.txt (line 7))\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/lib/python3.12/site-packages (from pytesseract->-r ../requirement.txt (line 9)) (23.2)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (from reportlab->-r ../requirement.txt (line 13)) (4.0.0)\n",
      "Requirement already satisfied: tinycss2>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from svglib->-r ../requirement.txt (line 14)) (1.2.1)\n",
      "Collecting cssselect2>=0.2.0 (from svglib->-r ../requirement.txt (line 14))\n",
      "  Using cached cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirement.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirement.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirement.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirement.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirement.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.12/site-packages (from cssselect2>=0.2.0->svglib->-r ../requirement.txt (line 14)) (0.5.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r ../requirement.txt (line 4))\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r ../requirement.txt (line 4))\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain->-r ../requirement.txt (line 3)) (1.33)\n",
      "Collecting pydantic<3,>=1 (from langchain->-r ../requirement.txt (line 3))\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain->-r ../requirement.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r ../requirement.txt (line 3)) (0.26.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r ../requirement.txt (line 3))\n",
      "  Downloading orjson-3.10.7-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->-r ../requirement.txt (line 5)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->-r ../requirement.txt (line 5)) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai->-r ../requirement.txt (line 5))\n",
      "  Downloading jiter-0.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->-r ../requirement.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->-r ../requirement.txt (line 5)) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1->langchain->-r ../requirement.txt (line 3)) (0.6.0)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain->-r ../requirement.txt (line 3))\n",
      "  Using cached pydantic_core-2.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->atlassian-python-api->-r ../requirement.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->atlassian-python-api->-r ../requirement.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->atlassian-python-api->-r ../requirement.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->atlassian-python-api->-r ../requirement.txt (line 2)) (2024.7.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai->-r ../requirement.txt (line 5)) (2023.10.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->atlassian-python-api->-r ../requirement.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from deprecated->atlassian-python-api->-r ../requirement.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r ../requirement.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r ../requirement.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain->-r ../requirement.txt (line 3)) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community->-r ../requirement.txt (line 4)) (1.0.0)\n",
      "Using cached atlassian_python_api-3.41.14-py3-none-any.whl (176 kB)\n",
      "Downloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.2.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.23-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pymongo-4.8.0-cp312-cp312-macosx_11_0_arm64.whl (698 kB)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Using cached reportlab-4.2.2-py3-none-any.whl (1.9 MB)\n",
      "Using cached cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Downloading langchain_core-0.2.35-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.106-py3-none-any.whl (150 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl (906 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading jiter-0.5.0-cp312-cp312-macosx_11_0_arm64.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.7-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.4/251.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: docx2txt, typing-inspect, reportlab, pytesseract, pydantic-core, pdf2image, orjson, oauthlib, marshmallow, jiter, dnspython, deprecated, tiktoken, requests-oauthlib, pymongo, pydantic, dataclasses-json, cssselect2, svglib, openai, langsmith, atlassian-python-api, langchain-core, langchain-text-splitters, langchain_openai, langchain, langchain_community\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "Successfully installed atlassian-python-api-3.41.14 cssselect2-0.7.0 dataclasses-json-0.6.7 deprecated-1.2.14 dnspython-2.6.1 docx2txt-0.8 jiter-0.5.0 langchain-0.2.15 langchain-core-0.2.35 langchain-text-splitters-0.2.2 langchain_community-0.2.13 langchain_openai-0.1.23 langsmith-0.1.106 marshmallow-3.22.0 oauthlib-3.2.2 openai-1.42.0 orjson-3.10.7 pdf2image-1.17.0 pydantic-2.8.2 pydantic-core-2.20.1 pymongo-4.8.0 pytesseract-0.3.13 reportlab-4.2.2 requests-oauthlib-2.0.0 svglib-1.5.1 tiktoken-0.7.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Confluence\n",
    "CONFLUENCE_TOKEN = os.getenv('CONFLUENCE_TOKEN')\n",
    "\n",
    "# Database\n",
    "COSMOSDB_VCORE_CONNECTION_STRING = os.getenv('COSMOSDB_VCORE_CONNECTION_STRING')\n",
    "COSMOSDB_NAMESPACE = os.getenv('COSMOSDB_NAMESPACE')\n",
    "\n",
    "# Azure OpenAI API\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "OPENAI_API_VERSION = os.getenv('OPENAI_API_VERSION')\n",
    "\n",
    "# LLM\n",
    "AZURE_OPENAI_LLM_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_LLM_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_LLM_MODEL_NAME = os.getenv('AZURE_OPENAI_LLM_MODEL_NAME')\n",
    "\n",
    "# Embedding\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_EMBEDDING_MODEL_NAME = os.getenv('AZURE_OPENAI_EMBEDDING_MODEL_NAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/confluence-Loader-vectorstore.png\" alt=\"confluence loader\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Confluence Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received runtime arguments {'space_key': 'SDV', 'include_attachments': False, 'limit': 1000, 'max_pages': 1000}. Passing runtime args to `load` is deprecated. Please pass arguments during initialization instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Introduction T-Systems brings a Global Presence, 5000 experts and a unique SDV technology acumen built in collaboration with car manufacturers over the past  years that enabled our team to design Cutting-Edge “Open Assets” T-Systems offers a unique SDV Solution Approach combining Services and “Open Assets” that is shortening time-to-market, matching any customer technology eco-systems, and empowering OEM with New Tech, IP and knowledge In collaboration with our sister company T-Mobile (DTAG), T-Systems bring together SDV’s 3 world, Back-end platform, Embedded Systems and Connectivity offering a unique End-to-End strategic prospective to OEM SDV Knowledge Base #303030 #303030 1 solid 40% Welcome to the SDV Knowledge Base 🤗 Here you can find valuable knowledge about our work and projects. The Purpose of this Knowledge Base is to make sure everyone gets the right knowledge at the right time. This can only function when everyone is contributing to knowledge sharing 🙏 So, feel free to leave your valuable knowledge as well and participate in this team approach. Search 60% 50% 50% 100% SDV Tribe: Setup and responsibilities true SDV Tribe 4MN4ASbFYcJLpehVQBQs false 800 72d86886fa0007f6a62bb2f07d02611033f3af97 auto top true 1200 39 #e20074 #ffffff #e20074 Domains #ffffff #af005a Embedded Apps & Services Committed to compliance, security, and innovation in the automotive industry's critical vehicle components, despite its dynamic evolution. #ffffff #af005a Over The Air Enable automotive OEMs to provide seamless OTA updates & adhere compliance. #ffffff #af005a Digital Apps & Platforms Offering comprehensive digital solutions and services for operating SDVs, with adaptable tools for OEMs, dealers and end-users alike. #ffffff #af005a ADAS/AD Driving force in tele-operated driving, data management, connectivity, and simulation, propelling self-driving vehicles to attain autonomy levels 3 and 4. #ffffff #af005a Telemetry & Connectivity Nerve center of SDVs, enabling them to perceive, connect, and communicate with the world around them - from vehicle data collection to analysis. #808080 #ffffff #808080 Solutions/Use Cases #ffffff #ffffff #666666 Intrusion Detection System The automotive industry is experiencing a surge in connected vehicles equipped with advanced communication technologies. As more vehicles become connected to the internet as well as external sensors and other vehicles, the potential risks from cybersecurity threats expand, making robust automotive security monitoring crucial in addition to it UNECE R155 regulation requires it. Monitors entire communication of InCar interfaces and bus systems for specific attack patterns ​ Compatible with Automotive Security Monitoring (ASM) by T-Systems ​ Analysis and measures according to customer needs ​ #ffffff #ffffff #666666 Consent Management Platform High complexity in managing numerous regulations from different regions for multi-brands and multiple applications. Manage the customer consents and trace all actions taken by customers via different channels. Centralised platform for multiple brands, but deployed regionally Build policy store (Bundle Permission) which shall be used for compound cases, as well as imply right to foget in a single click. Consent policy mapping at regional level & regional regulations for each user. Learn more... #ffffff #ffffff #666666 Feature On Demand Software releases are huge, stressful and mistakes cause significance financial damage to the OEM. Release cycles are very long and not easy to roll back. End customer wants to activate immediately certain features on demand Separation of software release and deployment (activation) Specific code or configurations can be enabled via a cloud service Integration possible via SDK or OTA Dynamic targeting based on vehicle attributes from DT. #ffffff #ffffff #666666 Over The Air Platform Difficulty in sourcing end-to-end solution. High switching xost between platforms and suppliers. Infrastructure Scalability issue: unexpected spikes in demand and balance traffic between multi-cloud infrastructures and application instances Our solution to OEMs OTA problems is focused on modularity, interoperability, scalability, and to be compliant with regulations. #ffffff #ffffff #666666 Vehicle Operation Center Mobility providers do not have a single view on all available data and information with the organization. The VOC is a one-stop-shop for fleet analysis and e nd-to-end lifecycle management of connected vehicles and platforms. The core of the VOC is our semantic layer architecture which helps in aggregating data from all available sources. With that, we create a 360° view on all available data which enables customers to identify issues and trigger operational actions. #ffffff #ffffff #666666 Digital Loop Bring validated and homologated software updates frequently and continuously to the ADAS/AD fleet.(UNECE 157 conform) Simulation as accepted and recognised evidence for technical reports pertaining to vehicle homologation. Traceability of continuous software updates throughout the entire product life cycle of a vehicle. #ffffff #ffffff #666666 Virtual Vehicle Simulation The logistical cost of using physical vehicle. The need for better tooling to ensure connected vehicle platform is working correctly Unavailability of OEM backend systems during early stage of testing leads to incomplete tests and errors in the later stages Create and deploy vehicles as well as fleets of vehicles, supporting all kinds of IoT protools like MQTT and LwM2M Execute tests and be flexible to allow manual unitary actions on virtual vehicles in a cloud-agnostic environment Simulate the intended behaviors/characteristics of physical vehicles and/or known disfunctionalities that need to be handled at platform level. Support communication with backend platform and applications for Authentication, Device Management, Data Collection, Remote Orders, etc. #ffffff #ffffff #666666 Diagnostics Customers seek to have high quality and effective diagnostic processes in production and service environment at reasonable costs. Goal is to reduce the time neede for diagnostics and in parallel improve the quality of the diagnostic result to avoid quality issues (zero failure in production) and improve customner satisfaction (service). #1AACA1 #ffffff #1AACA1 Business Services and Assets #ffffff #117870 Advisory Tailored guidance and advisory along the SDV domains OTA, Telemetry and Connectivity, ADAS/AD, Embedded Apps and Services and Platforms. From strategic insights to navigate challenges, ensuring your technologies align with industry trends. #ffffff #117870 Validation, Verification Assure quality and focusing on continuously improvement of capability, knowledge and skills based on test standards. With providing of testing blueprints, we balance and optimize the V&V process and lifecycle - S upply, D evelop, and V erify. #ffffff #117870 Security Elevate your solutions with our services, covering governance and compliance, architecture, tailored trainings, and process support. Benefit from hardware penetration tests up to a strategic roadmap to enhance your security posture - in vehicle as well as anywhere else. #ffffff #117870 Architecture Tailored architectural solutions to facilitate the evolution toward SDV. We design and implement flexible and scalable architectures, based on proven blueprints that enable cloud transformation, seamless integration of software, hardware and connectivity. #ffffff #117870 Cross Domain Services The glue in between all your services, enabling synergies and cost-efficient operations. Providing 24/7 comprehensive transparency, reliability as well as process support across the entire system landscape to connect all DevOps teams and stakeholders. #ffffff #117870 Data Intelligence & Analytics Providing highly scalable platforms for data processing and analytics of vehicle and machine data . ​ Enable easy access to data , data analytics and visualisation . #ffffff #117870 DevOps Develop and operate (next-gen) solution and services or taking over existing ones. From continuous integration, deployment to monitoring, we optimize your processes to enhance efficiency, reduce downtime, and ensure a seamless, agile lifecycle . #ffffff #117870 Domain / service-specific assets Hypercubes are reusable component software that can be deployed to create and enrich connected car platforms for our clients, primarily large automotive vehicle manufacturers . Hypercubes are designed to offer basic connected car functionalities, such as device management, OTA updates, or a vehicle digital twin. #1062AB #ffffff #1062AB Enablers #ffffff #0A4477 Knowledge & Skills A common space for knowledge management as well as describes processes, methodologies, responsibilities of how we do knowledge management within the SDV tribe. ​ Acts as a single source of truth for technical knowledge, tools and governance . #ffffff #0A4477 Business Planning Support function to monitor, control and  guide through the SDV – Governance model and also through the business case and cost & revenue realization. #ffffff #0A4477 GTM GTM continuously enables the fit between T-Systems SDV offering portfolio and market demands, over three dimensions: Value Proposition and Market Awareness and Customer Journey ​ . #ffffff #0A4477 Methodology & Compliance, Tools & Support Methodology &Compliance serves as a pathfinder through the jungle of regulations , standards and best practises , that need to be taken into account , when working in SDV area . Learn more ... #ffffff #0A4477 Partner Management Searching for partners for the three groups ( Product , Delivery , Marketing) to close gaps on the SDV landscape , prioritising and selecting partners (incl. contractual agreements ) . #ffffff #0A4477 Legal & Regulations Supporting, training of solution teams to develop accordingly to legal, regulatory requirements. ​ Supplying necessary tools, templates, training. ​ Learn more ... Do you want to contribute to this Knowledge Base and share your knowledge? #FFFFFF #FFFFFF #1AACA1 Note: Below are listed a few sample template. Click on one of the template, it will create a page with a few predefined fields. Start by filling those and make modifications as per your requirements. 130318349 130318349 Coding Content Template 130318350 130318350 Informative Content Template 130318351 130318351 Project Content Template Recent space activity page, comment, blogpost 5 true social Space contributors list descendants 5 true update' metadata={'title': 'SDV Tribe Home', 'id': '111314644', 'source': 'https://strive.devops.t-systems.net/confluence/display/SDV/SDV+Tribe+Home', 'when': '2024-07-09T12:16:03.574+02:00'}\n",
      "796 documents read from Confluence.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import ConfluenceLoader\n",
    "\n",
    "loader = ConfluenceLoader(\n",
    "    url=\"https://strive.devops.t-systems.net/confluence/\",\n",
    "    token=CONFLUENCE_TOKEN\n",
    ")\n",
    "\n",
    "confluence_documents = loader.load(\n",
    "    space_key=\"SDV\", \n",
    "    include_attachments=False, \n",
    "    limit=1000,\n",
    "    max_pages=1000,\n",
    ")\n",
    "\n",
    "print(confluence_documents[0])\n",
    "print(f'{len(confluence_documents)} documents read from Confluence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Confluence Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4151 chunks generated.\n",
      "page_content='Introduction T-Systems brings a Global Presence, 5000 experts and a unique SDV technology acumen built in collaboration with car manufacturers over the past  years that enabled our team to design Cutting-Edge “Open Assets” T-Systems offers a unique SDV Solution Approach combining Services and “Open Assets” that is shortening time-to-market, matching any customer technology eco-systems, and empowering OEM with New Tech, IP and knowledge In collaboration with our sister company T-Mobile (DTAG), T-Systems bring together SDV’s 3 world, Back-end platform, Embedded Systems and Connectivity offering a unique End-to-End strategic prospective to OEM SDV Knowledge Base #303030 #303030 1 solid 40% Welcome to the SDV Knowledge Base 🤗 Here you can find valuable knowledge about our work and projects. The Purpose of this Knowledge Base is to make sure everyone gets the right knowledge at the right time. This can only function when everyone is contributing to knowledge sharing 🙏 So, feel free to leave your valuable knowledge' metadata={'title': 'SDV Tribe Home', 'id': '111314644', 'source': 'https://strive.devops.t-systems.net/confluence/display/SDV/SDV+Tribe+Home', 'when': '2024-07-09T12:16:03.574+02:00'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024, \n",
    "    chunk_overlap = 300,\n",
    "    length_function = len)\n",
    "\n",
    "splitted_documents = text_splitter.split_documents(confluence_documents)\n",
    "\n",
    "print(f'{len(splitted_documents)} chunks generated.')\n",
    "print(splitted_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Openai Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x122d67590> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x1223445c0> model='text-embedding-ada-002' dimensions=None deployment='embedding-test-function' openai_api_version='2023-07-01-preview' openai_api_base=None openai_api_type='azure' openai_proxy='' embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1024 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True azure_endpoint='https://conmob-openai-deployments.openai.azure.com/' azure_ad_token=None azure_ad_token_provider=None validate_base_url=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    model=AZURE_OPENAI_EMBEDDING_MODEL_NAME,\n",
    "    embedding_ctx_length=8191,\n",
    "    chunk_size= 1024 \n",
    ")\n",
    "print(openai_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vectorstore.png\" alt=\"Example Image\" width=\"72\" height=\"72\">\n",
    "\n",
    "## Store Documents to COSMOSDB vCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gs/9bhdx00x6qdbv2vwzzm75fn80000gq/T/ipykernel_23370/295897153.py:10: UserWarning: You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb\n",
      "  client: MongoClient = MongoClient(COSMOSDB_VCORE_CONNECTION_STRING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'raw': {'defaultShard': {'numIndexesBefore': 1,\n",
       "   'numIndexesAfter': 2,\n",
       "   'createdCollectionAutomatically': False,\n",
       "   'ok': 1}},\n",
       " 'ok': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    ")\n",
    "from pymongo import MongoClient\n",
    "\n",
    "_indexName = 'km-index'\n",
    "_dbName, _collectionName = COSMOSDB_NAMESPACE.split(\".\")\n",
    "\n",
    "client: MongoClient = MongoClient(COSMOSDB_VCORE_CONNECTION_STRING)\n",
    "collection = client[_dbName][_collectionName]\n",
    "\n",
    "collection.database.drop_collection(_collectionName)\n",
    "\n",
    "vectorstore = AzureCosmosDBVectorSearch.from_documents(\n",
    "    splitted_documents,\n",
    "    openai_embeddings,\n",
    "    collection=collection,\n",
    "    index_name=_indexName,\n",
    ")\n",
    "\n",
    "num_lists = 100\n",
    "dimensions = 1536\n",
    "similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "\n",
    "vectorstore.create_index(num_lists, dimensions, similarity_algorithm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/llm.png\" alt=\"llm\" width=\"100\" height=\"100\">\n",
    "\n",
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 matching documents.\n",
      "Answer: The context does not provide information on who the first man on the moon was.\n",
      "SOURCES:\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "# Setup of the LLM and the chain\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_OPENAI_LLM_DEPLOYMENT_NAME, \n",
    "    model=AZURE_OPENAI_LLM_MODEL_NAME,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "chain = load_qa_with_sources_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "query = \"Does the context provide who the first man on the moon was?\"\n",
    "# query = \"Who is the team lead of knowledge management?\"\n",
    "# query = \"Who is Sirin Tiryaki?\"\n",
    "# query = \"What is Knowledge Management?\"\n",
    "# query = \"What is Knowledge Management? Explain in German.\"\n",
    "# query = \"Who is Jörg Tischler?\"\n",
    "# query = \"Who is the Business Unit Lead in Connected Mobility at T-Systems?\"\n",
    "# query = \"How can I book an external training?\"\n",
    "# query = \"Explain SDV to a 10-year-old.\"\n",
    "# query = \"what are hypercubes?\"\n",
    "# query = \"what are hypercubes?\"\n",
    "\n",
    "matching_docs = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "answer = chain.run(input_documents=matching_docs, question=query)\n",
    "\n",
    "print(f\"Found {len(matching_docs)} matching documents.\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
